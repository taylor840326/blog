## 多语言、中文分词及检索

-----

### 自然语言与查询Recall

当处理人类自然语言时，有些情况尽管搜索和原文不完全匹配，但是希望搜到一些内容

 Quick brown fox 和fast brow fox/ Jumping fox 和jumped foxes
 
 一些可采取的优化
 
 1. 归一化词元： 清楚变音符号，如ro^le的时候也会匹配role
 1. 抽取词根： 清楚单复数和时态的差异
 1. 包含同义词
 1. 拼写错误： 拼写错误，或者同音异形词
 
### 混合多语言的挑战

一些具体的多语言场景:

1. 不同的索引使用不同的语言
1. 同一个索引中，不同的字段使用不同的语言
1. 一个文档的一个字段内混合不同的语言

混合语言存在的一些挑战
 
1. 词干提取： 以色列文档，包含了希伯来语、阿拉伯语、俄语和英文
1. 不正确的文档频率。英文为主的文章中，德文算分高(稀有)
1. 需要判断用户搜索时使用的语言，语言识别(Compact Language Detectory)


### 分词的挑战

英文分词： You're 分成一个还是多个？ Half-baked

中文分词：

1. 分词标准： 哈工大标准中，姓和名分开。HanLP是在一起。具体情况需定制不同的标准。
1. 歧义： 组合型歧义、交集型歧义和真歧义

### 中文分词方法的演变- 字典法

查字段 - 最容易想到的分词方法。北京航空大学的梁南元教授提出

1. 一个句子从左导右扫描一遍。遇到有的词就标识出来。找到复合词，就找最长的。
1. 不认识的字符串就分割成单字词

最小词数的分词理论- 哈工大王晓龙博士把查字典的方法理论化

1. 一句话应该分成数量最少的词串
1. 遇到二义性分割，无能为力
1. 用各种文化规则来解决二义性，都并不成功

统计语言模型 - 1990年前后，清华大学电子工程系郭进博士

1. 解决了二义性问题，将中文分词的错误率降低了一个数量级。概率问题，动态规划+利用维特比算法快速找到最佳分词

基于统计的机器学习算法

1. 这类目前常用的算法是HMM,CRF,SVM,深度学习等算法。比如HanLP分词工具是基于CRF算法. 以CRF为例，基本思路是对汉字进行标注训练，不仅考虑了词语出现的频率，还考虑上下文，具备较好的学习能力，因此其对歧义词和未登陆词的识别具有良好的效果。
1. 随着深度学习的兴起，也出现了基于神经网络的分词器。有人尝试使用双向LSTM+CRF实现分词器，其本质上是序列标注，据报道其分词器字符准确率可高达97.6%


### 中文分词现状

中文分词器以统计语言模型未基础，经过几十年的发展，今天基本已经可以看作是一个已经解决的问题。

不同分词器的好坏，主要的差别在于数据的使用和工程使用的精度

常见的分词器都是使用机器学习算法和词典相结合，一方面能否提高分词准确率，另一方面能否改善领域适应性。

### 一些中文分词器

HanLP - 面向生产环境的自然语言处理工具包

HanLP分词器类型：

1. hanlp  - HanLP默认分词器
1. hanlp_standard - 标准分词器
1. hanlp_index - 索引分词
1. hanlp_nlp - nlp分词
1. hanlp_n_short - N-最短路分词
1. hanlp_dijkstra - 最短路分词器
1. hanlp_crf - CRF分词 (HanLP 1.6.6开始废弃)
1. hanlp_speed 急速词典分词器

IK分词器

Pinyin分词器

Java开源爬虫框架WebCollector设置代理
BY BRIEFCOPY · PUBLISHED 2016年4月25日 · UPDATED 2017年8月10日

WebCollector自带一个Proxys类，通过Proxys.nextRandom()方法可以随机获取加入的代理。
通过Proxys.add(“ip”,”端口号”);添加代理。
如果本机也参与http请求，可用Proxys.addEmpty()方法将本机加入。

覆盖Crawler的getResponse()方法，即可自定义使用随机代理的http请求：

@Override
public HttpResponse getResponse(CrawlDatum crawlDatum) throws Exception {
   HttpRequest request = new HttpRequest(crawlDatum);
   request.setProxy(proxys.nextRandom());
   return request.responsePage();
}
代码中的proxys并不是一个自带的对象，用户需要在继承Crawler时，定义一个成员变量：

Proxys proxys=new Proxys();
并且需要在构造函数或其他地方，向Proxys中添加代理服务器地址。

网上有很多免费代理网站，但这些代理非常不稳定。导致代理不稳定有两个原因，一方面，很多免费代理是通过黑客手段扫描出来的代理服务器，并不是专门的代理服务器，如果服务器的主人关闭服务器或代理服务，或者开启防火墙，代理自然就报废了；另一方面，提供免费代理列表的网站并不多，但使用免费代理列表的人却很多，很多都是写一个定时扫描程序，爬取免费代理网站，定时获取最新免费代理列表，一个代理服务器多人使用，速度得不到保障。

还有一些网站提供稳定的随机代理，但基本是收费的。

代理又可分为透明代理、普通匿名代理和高匿代理，这里引用网上一段解释：

透明代理的意思是客户端根本不需要知道有代理服务器的存在，但是它传送的仍然是真实的IP。你要想隐藏的话，不要用这个。

普通匿名代理能隐藏客户机的真实IP，但会改变我们的请求信息，服务器端有可能会认为我们使用了代理。不过使用此种代理时，虽然被访问的网站不能知道你的ip地址，但仍然可以知道你在使用代理，当然某些能够侦测ip的网页仍然可以查到你的ip。

高匿名代理不改变客户机的请求，这样在服务器看来就像有个真正的客户浏览器在访问它，这时客户的真实IP是隐藏的，服务器端不会认为我们使用了代理。

总的来说，还是高匿代理最为靠谱，不容易被网站的反爬虫机制察觉。